{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "929e3994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def create_coordinate_mesh(\n",
    "    coords: Dict[str, np.ndarray],\n",
    "    logger: logging.Logger\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create Cartesian product of all coordinate combinations.\n",
    "    \n",
    "    INPUTS:\n",
    "    - coords (dict): Dictionary with 'time', 'lat', 'lon' numpy arrays\n",
    "    - logger (logging.Logger): Logger instance\n",
    "    \n",
    "    OUTPUTS:\n",
    "    - pd.DataFrame: DataFrame with columns [time, lat, lon]\n",
    "    \n",
    "    FUNCTIONALITY:\n",
    "    Generates all possible combinations of (time, lat, lon) coordinates.\n",
    "    This creates the base structure for the flattened data.\n",
    "    \"\"\"\n",
    "    logger.debug(\"Creating coordinate mesh...\")\n",
    "    \n",
    "    # Get coordinate arrays\n",
    "    time_array = coords['time']\n",
    "    lat_array = coords['lat']\n",
    "    lon_array = coords['lon']\n",
    "    \n",
    "    # Calculate total number of points\n",
    "    n_time = len(time_array)\n",
    "    n_lat = len(lat_array)\n",
    "    n_lon = len(lon_array)\n",
    "    total_points = n_time * n_lat * n_lon\n",
    "    \n",
    "    logger.debug(f\"Generating {total_points:,} coordinate combinations\")\n",
    "    \n",
    "    # Create meshgrid for all coordinate combinations\n",
    "    # Using numpy's meshgrid for efficiency\n",
    "    time_mesh, lat_mesh, lon_mesh = np.meshgrid(\n",
    "        time_array, lat_array, lon_array,\n",
    "        indexing='ij'  # 'ij' indexing matches (time, lat, lon) order\n",
    "    )\n",
    "    \n",
    "    # Flatten meshgrids to 1D arrays\n",
    "    time_flat = time_mesh.ravel()\n",
    "    lat_flat = lat_mesh.ravel()\n",
    "    lon_flat = lon_mesh.ravel()\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'time': time_flat,\n",
    "        'lat': lat_flat,\n",
    "        'lon': lon_flat\n",
    "    })\n",
    "    \n",
    "    logger.debug(f\"Created coordinate mesh with {len(df):,} rows\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_variable_values(\n",
    "    dataset: xr.Dataset,\n",
    "    variable_name: str,\n",
    "    coord_names: Dict[str, str],\n",
    "    logger: logging.Logger\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract flattened values for a single variable.\n",
    "    \n",
    "    INPUTS:\n",
    "    - dataset (xr.Dataset): Opened xarray Dataset\n",
    "    - variable_name (str): Name of variable to extract\n",
    "    - coord_names (dict): Mapping of standard to actual coordinate names\n",
    "    - logger (logging.Logger): Logger instance\n",
    "    \n",
    "    OUTPUTS:\n",
    "    - np.ndarray: Flattened 1D array of variable values\n",
    "    \n",
    "    FUNCTIONALITY:\n",
    "    Extracts variable data, ensures proper dimension ordering (time, lat, lon),\n",
    "    and flattens to 1D array matching coordinate mesh order.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get variable data\n",
    "        var_data = dataset[variable_name]\n",
    "        \n",
    "        # Get actual coordinate names\n",
    "        time_name = coord_names.get('time', 'time')\n",
    "        lat_name = coord_names.get('lat', 'lat')\n",
    "        lon_name = coord_names.get('lon', 'lon')\n",
    "        \n",
    "        # Check if variable has the required dimensions\n",
    "        var_dims = list(var_data.dims)\n",
    "        \n",
    "        if not all(coord in var_dims for coord in [time_name, lat_name, lon_name]):\n",
    "            logger.warning(f\"Variable '{variable_name}' missing required dimensions. Skipping.\")\n",
    "            return None\n",
    "        \n",
    "        # Transpose to ensure (time, lat, lon) order\n",
    "        var_data = var_data.transpose(time_name, lat_name, lon_name)\n",
    "        \n",
    "        # Convert to numpy array and flatten\n",
    "        values = var_data.values.ravel()\n",
    "        \n",
    "        return values\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error extracting variable '{variable_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def append_variable_data(\n",
    "    df: pd.DataFrame,\n",
    "    dataset: xr.Dataset,\n",
    "    variable_names: List[str],\n",
    "    coord_names: Dict[str, str],\n",
    "    logger: logging.Logger,\n",
    "    show_progress: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Append all variable data columns to coordinate DataFrame.\n",
    "    \n",
    "    INPUTS:\n",
    "    - df (pd.DataFrame): Base DataFrame with time, lat, lon columns\n",
    "    - dataset (xr.Dataset): Opened xarray Dataset\n",
    "    - variable_names (list): List of variable names to extract\n",
    "    - coord_names (dict): Mapping of coordinate names\n",
    "    - logger (logging.Logger): Logger instance\n",
    "    - show_progress (bool): Show progress bar if True\n",
    "    \n",
    "    OUTPUTS:\n",
    "    - pd.DataFrame: DataFrame with all variables appended\n",
    "    \n",
    "    FUNCTIONALITY:\n",
    "    Iterates through all variables, extracts their values, and appends\n",
    "    as new columns to the DataFrame. Shows progress bar for tracking.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Extracting {len(variable_names)} variables...\")\n",
    "    \n",
    "    # Create progress bar if requested\n",
    "    iterator = tqdm(variable_names, desc=\"Extracting variables\") if show_progress else variable_names\n",
    "    \n",
    "    successful_vars = 0\n",
    "    failed_vars = []\n",
    "    \n",
    "    for var_name in iterator:\n",
    "        # Extract variable values\n",
    "        values = extract_variable_values(dataset, var_name, coord_names, logger)\n",
    "        \n",
    "        if values is not None:\n",
    "            # Verify length matches DataFrame\n",
    "            if len(values) == len(df):\n",
    "                df[var_name] = values\n",
    "                successful_vars += 1\n",
    "            else:\n",
    "                logger.warning(f\"Length mismatch for '{var_name}': \"\n",
    "                             f\"expected {len(df)}, got {len(values)}. Skipping.\")\n",
    "                failed_vars.append(var_name)\n",
    "        else:\n",
    "            failed_vars.append(var_name)\n",
    "    \n",
    "    logger.info(f\"Successfully extracted {successful_vars}/{len(variable_names)} variables\")\n",
    "    \n",
    "    if failed_vars:\n",
    "        logger.warning(f\"Failed to extract {len(failed_vars)} variables: {', '.join(failed_vars[:5])}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def flatten_dataset(\n",
    "    dataset: xr.Dataset,\n",
    "    coords: Dict[str, np.ndarray],\n",
    "    variable_names: List[str],\n",
    "    coord_names: Dict[str, str],\n",
    "    logger: logging.Logger,\n",
    "    show_progress: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main function to flatten 3D NetCDF dataset to 2D DataFrame.\n",
    "    \n",
    "    INPUTS:\n",
    "    - dataset (xr.Dataset): Opened xarray Dataset\n",
    "    - coords (dict): Dictionary with time, lat, lon arrays\n",
    "    - variable_names (list): List of variables to extract\n",
    "    - coord_names (dict): Mapping of coordinate names\n",
    "    - logger (logging.Logger): Logger instance\n",
    "    - show_progress (bool): Show progress bars if True\n",
    "    \n",
    "    OUTPUTS:\n",
    "    - pd.DataFrame: Flattened DataFrame with structure:\n",
    "                    [time, lat, lon, var1, var2, ..., varN]\n",
    "    \n",
    "    FUNCTIONALITY:\n",
    "    Orchestrates the complete flattening process:\n",
    "    1. Creates coordinate mesh\n",
    "    2. Extracts all variable values\n",
    "    3. Combines into single DataFrame\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting dataset flattening process...\")\n",
    "    \n",
    "    # Step 1: Create base coordinate mesh\n",
    "    df = create_coordinate_mesh(coords, logger)\n",
    "    \n",
    "    # Step 2: Append all variable data\n",
    "    df = append_variable_data(\n",
    "        df, dataset, variable_names, coord_names, logger, show_progress\n",
    "    )\n",
    "    \n",
    "    # Step 3: Ensure column order (time, lat, lon, then variables)\n",
    "    coord_cols = ['time', 'lat', 'lon']\n",
    "    var_cols = [col for col in df.columns if col not in coord_cols]\n",
    "    ordered_cols = coord_cols + sorted(var_cols)  # Sort variables alphabetically\n",
    "    df = df[ordered_cols]\n",
    "    \n",
    "    logger.info(f\"Flattening complete. DataFrame shape: {df.shape}\")\n",
    "    logger.info(f\"Columns: {len(df.columns)} (3 coordinates + {len(var_cols)} variables)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def optimize_dataframe_memory(\n",
    "    df: pd.DataFrame,\n",
    "    logger: logging.Logger\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Optimize DataFrame memory usage by converting data types.\n",
    "    \n",
    "    INPUTS:\n",
    "    - df (pd.DataFrame): DataFrame to optimize\n",
    "    - logger (logging.Logger): Logger instance\n",
    "    \n",
    "    OUTPUTS:\n",
    "    - pd.DataFrame: Optimized DataFrame\n",
    "    \n",
    "    FUNCTIONALITY:\n",
    "    Converts float64 to float32 where appropriate to reduce memory usage.\n",
    "    Useful for very large datasets.\n",
    "    \"\"\"\n",
    "    logger.debug(\"Optimizing DataFrame memory usage...\")\n",
    "    \n",
    "    memory_before = df.memory_usage(deep=True).sum() / 1024**2  # MB\n",
    "    \n",
    "    # Convert float64 columns to float32 (except coordinates)\n",
    "    for col in df.columns:\n",
    "        if col not in ['time', 'lat', 'lon'] and df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "    \n",
    "    memory_after = df.memory_usage(deep=True).sum() / 1024**2  # MB\n",
    "    reduction = ((memory_before - memory_after) / memory_before) * 100\n",
    "    \n",
    "    logger.debug(f\"Memory reduced from {memory_before:.1f} MB to {memory_after:.1f} MB \"\n",
    "                f\"({reduction:.1f}% reduction)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_empty_variable_rows(\n",
    "    df: pd.DataFrame,\n",
    "    logger: logging.Logger\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove rows where ANY variable column is empty/NaN.\n",
    "    \n",
    "    INPUTS:\n",
    "    - df (pd.DataFrame): DataFrame to clean\n",
    "    - logger (logging.Logger): Logger instance\n",
    "    \n",
    "    OUTPUTS:\n",
    "    - pd.DataFrame: Cleaned DataFrame with incomplete rows removed\n",
    "    \n",
    "    FUNCTIONALITY:\n",
    "    Identifies rows where ANY data variable (excluding time, lat, lon)\n",
    "    is NaN or missing, and removes them from the DataFrame.\n",
    "    Only keeps rows where ALL variables have valid values.\n",
    "    \"\"\"\n",
    "    logger.info(\"Removing rows with any empty variables...\")\n",
    "    \n",
    "    rows_before = len(df)\n",
    "    \n",
    "    # Get variable columns (exclude coordinates)\n",
    "    coord_cols = ['time', 'lat', 'lon']\n",
    "    var_cols = [col for col in df.columns if col not in coord_cols]\n",
    "    \n",
    "    if not var_cols:\n",
    "        logger.warning(\"No variable columns found to check for empty rows\")\n",
    "        return df\n",
    "    \n",
    "    # Remove rows where ANY variable column is NaN (UPDATED)\n",
    "    # Keep row only if all variables have values\n",
    "    df_cleaned = df.dropna(subset=var_cols, how='any')\n",
    "    \n",
    "    rows_after = len(df_cleaned)\n",
    "    rows_removed = rows_before - rows_after\n",
    "    \n",
    "    if rows_removed > 0:\n",
    "        logger.info(f\"Removed {rows_removed:,} rows with any empty variables \"\n",
    "                   f\"({(rows_removed/rows_before*100):.1f}% of data)\")\n",
    "        logger.info(f\"Remaining rows: {rows_after:,}\")\n",
    "    else:\n",
    "        logger.info(\"No incomplete rows found - all data retained\")\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "def process_date_features(\n",
    "    df: pd.DataFrame,\n",
    "    logger: logging.Logger\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert time column to date and hour features with cyclical encoding.\n",
    "    \n",
    "    INPUTS:\n",
    "    - df (pd.DataFrame): DataFrame with 'time' column\n",
    "    - logger (logging.Logger): Logger instance\n",
    "    \n",
    "    OUTPUTS:\n",
    "    - pd.DataFrame: DataFrame with date, hour, and cyclical hour encoding\n",
    "    \n",
    "    FUNCTIONALITY:\n",
    "    Creates the following features from 'time' column:\n",
    "    - date: Date only (YYYY-MM-DD format, no time)\n",
    "    - hour: Hour in format 00, 03, 06, 09, 12, 15, 18, or 21\n",
    "    - sin_hour: Sine component of hour (cyclical encoding)\n",
    "    - cos_hour: Cosine component of hour (cyclical encoding)\n",
    "    - sin_day_of_year: Sine component of day of year (cyclical encoding)\n",
    "    - cos_day_of_year: Cosine component of day of year (cyclical encoding)\n",
    "    - sin_month: Sine component of month (cyclical encoding)\n",
    "    - cos_month: Cosine component of month (cyclical encoding)\n",
    "    - sin_day_of_week: Sine component of day of week (cyclical encoding)\n",
    "    - cos_day_of_week: Cosine component of day of week (cyclical encoding)\n",
    "    \n",
    "    Then removes the original 'time' column.\n",
    "    \"\"\"\n",
    "    logger.info(\"Processing date features from time column...\")\n",
    "    \n",
    "    # Check if time column exists\n",
    "    if 'time' not in df.columns:\n",
    "        logger.warning(\"No 'time' column found - skipping date processing\")\n",
    "        return df\n",
    "    \n",
    "    # Convert time column to datetime if not already\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # Extract date only (no time component) - format as string YYYY-MM-DD\n",
    "    df['date'] = df['time'].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Extract hour and format as 00, 03, 06, 09, 12, 15, 18, 21\n",
    "    df['hour'] = df['time'].dt.hour.astype(str).str.zfill(2)\n",
    "    \n",
    "    # Get numeric hour for cyclical encoding\n",
    "    hour_numeric = df['time'].dt.hour\n",
    "    \n",
    "    # Cyclical encoding for hour (24-hour cycle)\n",
    "    df['sin_hour'] = np.sin(2 * np.pi * hour_numeric / 24)\n",
    "    df['cos_hour'] = np.cos(2 * np.pi * hour_numeric / 24)\n",
    "    \n",
    "    # Extract temporal features for cyclical encoding\n",
    "    day_of_year = df['time'].dt.dayofyear\n",
    "    month = df['time'].dt.month\n",
    "    day_of_week = df['time'].dt.dayofweek\n",
    "    \n",
    "    # Cyclical encoding for day of year (handles leap years)\n",
    "    df['sin_day_of_year'] = np.sin(2 * np.pi * day_of_year / 366)\n",
    "    df['cos_day_of_year'] = np.cos(2 * np.pi * day_of_year / 366)\n",
    "    \n",
    "    # Cyclical encoding for month\n",
    "    df['sin_month'] = np.sin(2 * np.pi * month / 12)\n",
    "    df['cos_month'] = np.cos(2 * np.pi * month / 12)\n",
    "    \n",
    "    # Cyclical encoding for day of week\n",
    "    df['sin_day_of_week'] = np.sin(2 * np.pi * day_of_week / 7)\n",
    "    df['cos_day_of_week'] = np.cos(2 * np.pi * day_of_week / 7)\n",
    "    \n",
    "    # Reorder columns: lat, lon, date, hour, cyclical encodings, then variable columns\n",
    "    coord_cols = ['lat', 'lon']\n",
    "    date_cols = [\n",
    "        'date', 'hour',\n",
    "        'sin_hour', 'cos_hour',\n",
    "        'sin_day_of_year', 'cos_day_of_year',\n",
    "        'sin_month', 'cos_month',\n",
    "        'sin_day_of_week', 'cos_day_of_week'\n",
    "    ]\n",
    "    \n",
    "    # Get variable columns (all columns except time, lat, lon, and new date columns)\n",
    "    all_cols = set(df.columns)\n",
    "    exclude_cols = set(['time'] + coord_cols + date_cols)\n",
    "    var_cols = sorted(list(all_cols - exclude_cols))\n",
    "    \n",
    "    # Reorder: coordinates, date features, then variables\n",
    "    ordered_cols = coord_cols + date_cols + var_cols\n",
    "    df = df[ordered_cols]\n",
    "    \n",
    "    logger.info(f\"Date features created: {len(date_cols)} new columns added, 'time' column removed\")\n",
    "    logger.debug(f\"New date columns: {', '.join(date_cols)}\")\n",
    "    logger.info(f\"Final column order: lat, lon, date features ({len(date_cols)}), \" +\n",
    "               f\"variables ({len(var_cols)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e17135fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df[[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhour2\u001b[39m\u001b[33m\"\u001b[39m]] = \u001b[43mdf\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m].str.split(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m, expand=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      2\u001b[39m df = df.drop(columns=[\u001b[33m\"\u001b[39m\u001b[33mhour2\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df[[\"date\", \"hour2\"]] = df[\"date\"].str.split(\" \", expand=True)\n",
    "df = df.drop(columns=[\"hour2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
